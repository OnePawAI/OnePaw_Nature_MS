{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f272d5-08b9-47a9-b4a3-06647df89ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio rdkit-pypi datasets selfies tokenizers tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a82c5e8-9478-4d95-b422-c28c7523bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nature_msms\n",
    "\n",
    "#SOLVED variable issue\n",
    "\n",
    "# Import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import selfies as sf\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define token variables early\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "# Load and preprocess dataset\n",
    "dataset = load_dataset('/kaggle/input/massdata', split='train')  # Use full dataset\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Inspect dataset\n",
    "print(\"Dataset Columns:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df[['identifier', 'mzs', 'intensities', 'smiles', 'adduct', 'precursor_mz']].head())\n",
    "print(\"\\nUnique adduct values:\", df['adduct'].unique())\n",
    "\n",
    "# Binning spectra\n",
    "def bin_spectrum(mzs, intensities, n_bins=1000, max_mz=1000):\n",
    "    spectrum = np.zeros(n_bins)\n",
    "    for mz, intensity in zip(mzs, intensities):\n",
    "        try:\n",
    "            mz = float(mz)\n",
    "            intensity = float(intensity)\n",
    "            if mz < max_mz:\n",
    "                bin_idx = int((mz / max_mz) * n_bins)\n",
    "                spectrum[bin_idx] += intensity\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    if spectrum.max() > 0:\n",
    "        spectrum = spectrum / spectrum.max()\n",
    "    return spectrum\n",
    "\n",
    "df['binned'] = df.apply(lambda row: bin_spectrum(row['mzs'], row['intensities']), axis=1)\n",
    "\n",
    "# Convert SMILES to SELFIES\n",
    "df['selfies'] = df['smiles'].apply(lambda s: sf.encoder(s) if Chem.MolFromSmiles(s) else None)\n",
    "df = df.dropna(subset=['selfies'])  # Drop invalid SMILES/SELFIES\n",
    "\n",
    "# Preprocess ion mode (from adduct) and precursor m/z\n",
    "df['ion_mode'] = df['adduct'].apply(lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0).fillna(0)\n",
    "df['precursor_bin'] = pd.qcut(df['precursor_mz'], q=100, labels=False, duplicates='drop')\n",
    "\n",
    "# Train-validation split\n",
    "df_train, df_val = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Verify preprocessing\n",
    "print(\"\\nFirst few rows of preprocessed data:\")\n",
    "print(df[['identifier', 'binned', 'selfies', 'ion_mode', 'precursor_bin']].head())\n",
    "\n",
    "# Check maximum SELFIES length\n",
    "max_len = max(len(list(sf.split_selfies(s)) + [SOS_TOKEN, EOS_TOKEN]) for s in df['selfies'])\n",
    "print(f\"Maximum SELFIES length: {max_len}\")\n",
    "\n",
    "# Tokenization\n",
    "all_selfies = df_train['selfies'].tolist()\n",
    "unique_tokens = set()\n",
    "for sf_str in all_selfies:\n",
    "    tokens = list(sf.split_selfies(sf_str))  # Convert generator to list\n",
    "    unique_tokens.update(tokens)\n",
    "\n",
    "tokens = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN] + sorted(unique_tokens)\n",
    "token_to_idx = {tok: i for i, tok in enumerate(tokens)}\n",
    "idx_to_token = {i: tok for tok, i in token_to_idx.items()}\n",
    "vocab_size = len(tokens)\n",
    "MAX_LEN = max(150, max_len)  # Adjust MAX_LEN based on dataset\n",
    "\n",
    "def encode_selfies(sf_string):\n",
    "    tokens = [SOS_TOKEN] + list(sf.split_selfies(sf_string)) + [EOS_TOKEN]\n",
    "    token_ids = [token_to_idx[tok] for tok in tokens if tok in token_to_idx]\n",
    "    if len(token_ids) > MAX_LEN:\n",
    "        token_ids = token_ids[:MAX_LEN]\n",
    "    else:\n",
    "        token_ids += [token_to_idx[PAD_TOKEN]] * (MAX_LEN - len(token_ids))\n",
    "    return token_ids\n",
    "\n",
    "# Dataset class\n",
    "class MSMSDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.spectra = np.stack(dataframe['binned'].values)\n",
    "        self.selfies = [encode_selfies(s) for s in dataframe['selfies']]\n",
    "        self.ion_modes = dataframe['ion_mode'].values\n",
    "        self.precursor_bins = dataframe['precursor_bin'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spectra)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.spectra[idx], dtype=torch.float),\n",
    "            torch.tensor(self.selfies[idx], dtype=torch.long),\n",
    "            torch.tensor(self.ion_modes[idx], dtype=torch.long),\n",
    "            torch.tensor(self.precursor_bins[idx], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "train_dataset = MSMSDataset(df_train)\n",
    "val_dataset = MSMSDataset(df_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, num_workers=2)\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# Metadata Embedding\n",
    "class SpectrumMetadataEmbedding(nn.Module):\n",
    "    def __init__(self, emb_dim=64, ion_mode_dim=2, precursor_bins=100):\n",
    "        super().__init__()\n",
    "        self.ion_emb = nn.Embedding(ion_mode_dim, emb_dim)\n",
    "        self.prec_emb = nn.Embedding(precursor_bins, emb_dim)\n",
    "        self.linear = nn.Linear(2 * emb_dim, emb_dim)\n",
    "\n",
    "    def forward(self, ion_mode_idx, precursor_idx):\n",
    "        ion_vec = self.ion_emb(ion_mode_idx)\n",
    "        prec_vec = self.prec_emb(precursor_idx)\n",
    "        combined = torch.cat([ion_vec, prec_vec], dim=-1)\n",
    "        return self.linear(combined)\n",
    "\n",
    "# Transformer Encoder\n",
    "class SpectrumTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=1000, d_model=512, nhead=8, num_layers=6, dim_feedforward=1024, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.metadata_emb = SpectrumMetadataEmbedding(emb_dim=64)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(d_model + 64, d_model)\n",
    "\n",
    "    def forward(self, src, ion_mode_idx, precursor_idx):\n",
    "        src = self.input_proj(src).unsqueeze(1)  # Shape: (batch_size, 1, d_model)\n",
    "        metadata = self.metadata_emb(ion_mode_idx, precursor_idx)  # Shape: (batch_size, emb_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src).squeeze(1)  # Shape: (batch_size, d_model)\n",
    "        output = torch.cat([output, metadata], dim=-1)  # Concat metadata\n",
    "        output = self.fc(output)  # Shape: (batch_size, d_model)\n",
    "        return output\n",
    "\n",
    "# Transformer Decoder\n",
    "class SelfiesTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, dim_feedforward=1024, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_key_padding_mask=None):\n",
    "        embedded = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        output = self.transformer_decoder(embedded, memory, tgt_mask=tgt_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return self.output_layer(output)\n",
    "\n",
    "# Full Transformer Model\n",
    "class MSMS2SelfiesTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, dim_feedforward=1024, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.encoder = SpectrumTransformerEncoder(input_dim=1000, d_model=d_model, nhead=nhead, num_layers=num_layers, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.decoder = SelfiesTransformerDecoder(vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, tgt_len):\n",
    "        mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1)\n",
    "        mask = mask.float().masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt, ion_mode_idx, precursor_idx, tgt_mask=None, memory_key_padding_mask=None):\n",
    "        memory = self.encoder(src, ion_mode_idx, precursor_idx).unsqueeze(1)  # Shape: (batch_size, 1, d_model)\n",
    "        output = self.decoder(tgt, memory, tgt_mask, memory_key_padding_mask)\n",
    "        return output\n",
    "\n",
    "# SSL Pretraining for Encoder\n",
    "def mask_spectrum(spectrum, mask_ratio=0.15):\n",
    "    spectrum = spectrum.clone()\n",
    "    n_mask = int(mask_ratio * spectrum.size(0))\n",
    "    mask_indices = torch.randperm(spectrum.size(0))[:n_mask]\n",
    "    spectrum[mask_indices] = 0\n",
    "    return spectrum\n",
    "\n",
    "def ssl_pretrain_encoder(encoder, dataloader, epochs=3, lr=1e-4):\n",
    "    encoder.train()\n",
    "    optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for spectra, _, ion_modes, precursor_bins in tqdm(dataloader, desc=f\"SSL Epoch {epoch+1}/{epochs}\"):\n",
    "            spectra = spectra.to(device)\n",
    "            ion_modes = ion_modes.to(device)\n",
    "            precursor_bins = precursor_bins.to(device)\n",
    "            masked_spectra = torch.stack([mask_spectrum(s) for s in spectra]).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = encoder(masked_spectra, ion_modes, precursor_bins)\n",
    "            loss = criterion(reconstructed, encoder(spectra, ion_modes, precursor_bins))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"SSL Epoch {epoch+1}/{epochs} - Reconstruction Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Supervised Training with Early Stopping\n",
    "def supervised_train(model, train_loader, val_loader, epochs=10, lr=1e-4, patience=3):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=token_to_idx[PAD_TOKEN])\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for spectra, selfies_tokens, ion_modes, precursor_bins in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            spectra, selfies_tokens = spectra.to(device), selfies_tokens.to(device)\n",
    "            ion_modes, precursor_bins = ion_modes.to(device), precursor_bins.to(device)\n",
    "            tgt_input = selfies_tokens[:, :-1]\n",
    "            tgt_output = selfies_tokens[:, 1:]\n",
    "            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "            memory_key_padding_mask = None  # No padding in memory since src_len=1\n",
    "            optimizer.zero_grad()\n",
    "            output = model(spectra, tgt_input, ion_modes, precursor_bins, tgt_mask, memory_key_padding_mask)\n",
    "            loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for spectra, selfies_tokens, ion_modes, precursor_bins in val_loader:\n",
    "                spectra, selfies_tokens = spectra.to(device), selfies_tokens.to(device)\n",
    "                ion_modes, precursor_bins = ion_modes.to(device), precursor_bins.to(device)\n",
    "                tgt_input = selfies_tokens[:, :-1]\n",
    "                tgt_output = selfies_tokens[:, 1:]\n",
    "                tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "                memory_key_padding_mask = None\n",
    "                output = model(spectra, tgt_input, ion_modes, precursor_bins, tgt_mask, memory_key_padding_mask)\n",
    "                loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'token_to_idx': token_to_idx,\n",
    "                'idx_to_token': idx_to_token\n",
    "            }, 'best_msms_transformer.pt')\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# Beam Search Inference with Diversity Penalty\n",
    "def beam_search(model, spectrum, ion_mode_idx, precursor_idx, beam_width=10, max_len=150, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        spectrum = spectrum.unsqueeze(0).to(device)\n",
    "        ion_mode_idx = torch.tensor([ion_mode_idx], dtype=torch.long).to(device)\n",
    "        precursor_idx = torch.tensor([precursor_idx], dtype=torch.long).to(device)\n",
    "        memory = model.encoder(spectrum, ion_mode_idx, precursor_idx).unsqueeze(1)  # Shape: (1, 1, d_model)\n",
    "        sequences = [([token_to_idx[SOS_TOKEN]], 0.0)]  # (sequence, log_prob)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for seq, score in sequences:\n",
    "                if seq[-1] == token_to_idx[EOS_TOKEN]:\n",
    "                    all_candidates.append((seq, score))\n",
    "                    continue\n",
    "                tgt_input = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "                tgt_mask = model.generate_square_subsequent_mask(len(seq)).to(device)\n",
    "                outputs = model.decoder(tgt_input, memory, tgt_mask)\n",
    "                log_probs = F.log_softmax(outputs[0, -1], dim=-1).cpu().numpy()\n",
    "                top_tokens = np.argsort(log_probs)[-beam_width:]\n",
    "                for tok in top_tokens:\n",
    "                    diversity_penalty = 0.1 * sum(1 for s, _ in sequences if tok in s[1:-1])  # Penalize repeated tokens\n",
    "                    candidate = (seq + [tok], score + log_probs[tok] - diversity_penalty)\n",
    "                    all_candidates.append(candidate)\n",
    "            sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            if all(seq[-1] == token_to_idx[EOS_TOKEN] for seq, _ in sequences):\n",
    "                break\n",
    "\n",
    "        results = []\n",
    "        for seq, score in sequences:\n",
    "            sf_str = ''.join([idx_to_token.get(idx, '') for idx in seq[1:-1]])\n",
    "            try:\n",
    "                smiles = sf.decoder(sf_str)\n",
    "                if Chem.MolFromSmiles(smiles):\n",
    "                    confidence = np.exp(score / len(seq))  # Normalized score\n",
    "                    results.append((smiles, confidence))\n",
    "            except:\n",
    "                continue\n",
    "        return results if results else [(\"Invalid SMILES\", 0.0)]\n",
    "\n",
    "# Tanimoto Similarity\n",
    "def tanimoto_similarity(smiles1, smiles2):\n",
    "    mol1 = Chem.MolFromSmiles(smiles1)\n",
    "    mol2 = Chem.MolFromSmiles(smiles2)\n",
    "    if mol1 and mol2:\n",
    "        fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 2, 2048)\n",
    "        fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, 2, 2048)\n",
    "        return Chem.DataStructs.TanimotoSimilarity(fp1, fp2)\n",
    "    return 0.0\n",
    "\n",
    "# Initialize and Train Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MSMS2SelfiesTransformer(vocab_size=vocab_size, d_model=512, num_layers=6, dim_feedforward=1024, dropout=0.2).to(device)\n",
    "\n",
    "# SSL Pretraining\n",
    "print(\"Starting SSL pretraining...\")\n",
    "ssl_pretrain_encoder(model.encoder, train_loader, epochs=3)\n",
    "\n",
    "# Supervised Training\n",
    "print(\"Starting supervised training...\")\n",
    "best_val_loss = supervised_train(model, train_loader, val_loader, epochs=10, patience=3)\n",
    "\n",
    "print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "print(\"Model saved as 'best_msms_transformer.pt'\")\n",
    "\n",
    "# Inference and Visualization\n",
    "sample_idx = 0\n",
    "sample_spectrum = torch.tensor(df_val['binned'].iloc[sample_idx], dtype=torch.float)\n",
    "sample_ion_mode = df_val['ion_mode'].iloc[sample_idx]\n",
    "sample_precursor_bin = df_val['precursor_bin'].iloc[sample_idx]\n",
    "true_smiles = df_val['smiles'].iloc[sample_idx]\n",
    "\n",
    "predicted_results = beam_search(model, sample_spectrum, sample_ion_mode, sample_precursor_bin, beam_width=10, device=device)\n",
    "print(f\"True SMILES: {true_smiles}\")\n",
    "print(\"Top Predicted SMILES:\")\n",
    "for smiles, confidence in predicted_results[:3]:\n",
    "    print(f\"SMILES: {smiles}, Confidence: {confidence:.4f}\")\n",
    "    similarity = tanimoto_similarity(true_smiles, smiles)\n",
    "    print(f\"Tanimoto Similarity: {similarity:.4f}\")\n",
    "    if len(smiles) > 100 and smiles.count('C') > len(smiles) * 0.8:\n",
    "        print(\"Warning: Predicted SMILES is a long carbon chain, indicating potential model underfitting.\")\n",
    "\n",
    "# Visualize molecules\n",
    "if predicted_results[0][0] != \"Invalid SMILES\":\n",
    "    pred_mol = Chem.MolFromSmiles(predicted_results[0][0])\n",
    "    true_mol = Chem.MolFromSmiles(true_smiles)\n",
    "    if pred_mol and true_mol:\n",
    "        img = Draw.MolsToGridImage([true_mol, pred_mol], molsPerRow=2, subImgSize=(300, 300), legends=['True', 'Predicted'])\n",
    "        img_array = np.array(img.convert('RGB'))  # Convert PIL Image to RGB and then to NumPy array\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(img_array)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c40faa-8305-4335-96d1-cf5355c80d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
