{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd0Hc_ItxPrz"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio rdkit datasets tokenizers tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#final_version\n",
        "# stereochemistry_fixed\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.data import Data, Batch\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw, Descriptors, rdFMCS, EnumerateStereoisomers\n",
        "from rdkit import DataStructs\n",
        "from rdkit.Chem import rdFingerprintGenerator\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import optuna\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from Levenshtein import distance\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define token variables early\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "SOS_TOKEN = \"<SOS>\"\n",
        "EOS_TOKEN = \"<EOS>\"\n",
        "MASK_TOKEN = \"[MASK]\"\n",
        "\n",
        "# Load and preprocess dataset\n",
        "dataset = load_dataset('roman-bushuiev/MassSpecGym', split='train')\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# Simulate external dataset (e.g., NIST-like) by splitting\n",
        "df_massspecgym, df_external = df.iloc[:int(0.9*len(df))], df.iloc[int(0.9*len(df)):]\n",
        "print(\"MassSpecGym size:\", len(df_massspecgym), \"External test size:\", len(df_external))\n",
        "\n",
        "# Inspect dataset\n",
        "print(\"Dataset Columns:\", df_massspecgym.columns.tolist())\n",
        "print(\"\\nFirst few rows of MassSpecGym dataset:\")\n",
        "print(df_massspecgym[['identifier', 'mzs', 'intensities', 'smiles', 'adduct', 'precursor_mz']].head())\n",
        "print(\"\\nUnique adduct values:\", df_massspecgym['adduct'].unique())\n",
        "\n",
        "# Data augmentation: SMILES enumeration and spectral noise\n",
        "def augment_smiles(smiles):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol:\n",
        "            stereoisomers = EnumerateStereoisomers.EnumerateStereoisomers(mol)\n",
        "            return [Chem.MolToSmiles(m, canonical=True, doRandom=True) for m in stereoisomers]\n",
        "        return [smiles]\n",
        "    except:\n",
        "        return [smiles]\n",
        "\n",
        "def bin_spectrum_to_graph(mzs, intensities, ion_mode, precursor_mz, adduct, n_bins=1000, max_mz=1000, noise_level=0.05):\n",
        "    spectrum = np.zeros(n_bins)\n",
        "    for mz, intensity in zip(mzs, intensities):\n",
        "        try:\n",
        "            mz = float(mz)\n",
        "            intensity = float(intensity)\n",
        "            if mz < max_mz:\n",
        "                bin_idx = int((mz / max_mz) * n_bins)\n",
        "                spectrum[bin_idx] += intensity\n",
        "        except (ValueError, TypeError):\n",
        "            continue\n",
        "    if spectrum.max() > 0:\n",
        "        spectrum = spectrum / spectrum.max()\n",
        "    spectrum += np.random.normal(0, noise_level, spectrum.shape).clip(0, 1)\n",
        "    x = torch.tensor(spectrum, dtype=torch.float).unsqueeze(-1)\n",
        "    edge_index = []\n",
        "    for i in range(n_bins-1):\n",
        "        edge_index.append([i, i+1])\n",
        "        edge_index.append([i+1, i])\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
        "    ion_mode = torch.tensor([ion_mode], dtype=torch.float)\n",
        "    precursor_mz = torch.tensor([precursor_mz], dtype=torch.float)\n",
        "    adduct_idx = adduct_to_idx.get(adduct, 0)\n",
        "    return spectrum, Data(x=x, edge_index=edge_index, ion_mode=ion_mode, precursor_mz=precursor_mz, adduct_idx=adduct_idx)\n",
        "\n",
        "# Canonicalize SMILES and augment\n",
        "def canonicalize_smiles(smiles):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
        "        if mol:\n",
        "            return Chem.MolToSmiles(mol, canonical=True)\n",
        "        return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "df_massspecgym['smiles'] = df_massspecgym['smiles'].apply(canonicalize_smiles)\n",
        "df_external['smiles'] = df_external['smiles'].apply(canonicalize_smiles)\n",
        "df_massspecgym = df_massspecgym.dropna(subset=['smiles'])\n",
        "df_external = df_external.dropna(subset=['smiles'])\n",
        "df_massspecgym['smiles_list'] = df_massspecgym['smiles'].apply(augment_smiles)\n",
        "df_massspecgym = df_massspecgym.explode('smiles_list').dropna(subset=['smiles_list']).rename(columns={'smiles_list': 'smiles'})\n",
        "\n",
        "# Preprocess ion mode, precursor m/z, and adducts\n",
        "df_massspecgym['ion_mode'] = df_massspecgym['adduct'].apply(lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0).fillna(0)\n",
        "df_massspecgym['precursor_bin'] = pd.qcut(df_massspecgym['precursor_mz'], q=100, labels=False, duplicates='drop')\n",
        "df_external['ion_mode'] = df_external['adduct'].apply(lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0).fillna(0)\n",
        "df_external['precursor_bin'] = pd.qcut(df_external['precursor_mz'], q=100, labels=False, duplicates='drop')\n",
        "adduct_types = df_massspecgym['adduct'].unique()\n",
        "adduct_to_idx = {adduct: i for i, adduct in enumerate(adduct_types)}\n",
        "df_massspecgym['adduct_idx'] = df_massspecgym['adduct'].map(adduct_to_idx)\n",
        "df_external['adduct_idx'] = df_external['adduct'].map(adduct_to_idx)\n",
        "\n",
        "df_massspecgym[['binned', 'graph_data']] = df_massspecgym.apply(\n",
        "    lambda row: pd.Series(bin_spectrum_to_graph(row['mzs'], row['intensities'], row['ion_mode'], row['precursor_mz'], row['adduct'])),\n",
        "    axis=1\n",
        ")\n",
        "df_external[['binned', 'graph_data']] = df_external.apply(\n",
        "    lambda row: pd.Series(bin_spectrum_to_graph(row['mzs'], row['intensities'], row['ion_mode'], row['precursor_mz'], row['adduct'])),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# SMILES Tokenization with Stereochemistry\n",
        "all_smiles = df_massspecgym['smiles'].tolist()\n",
        "unique_chars = set(''.join(all_smiles)) | {MASK_TOKEN}\n",
        "valid_atoms = {'C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I', 'H'}\n",
        "tokens = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, MASK_TOKEN] + sorted(unique_chars - {PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, MASK_TOKEN})\n",
        "token_to_idx = {tok: i for i, tok in enumerate(tokens) if tok in valid_atoms or tok in {PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, MASK_TOKEN, '(', ')', '=', '#', '@', '[', ']', '/', '\\\\', '.', ':'}}\n",
        "idx_to_token = {i: tok for tok, i in token_to_idx.items()}\n",
        "vocab_size = len(token_to_idx)\n",
        "PRETRAIN_MAX_LEN = 100\n",
        "SUPERVISED_MAX_LEN = max(len(s) + 2 for s in all_smiles)\n",
        "print(f\"Vocabulary size: {vocab_size}, Supervised MAX_LEN: {SUPERVISED_MAX_LEN}, Pretrain MAX_LEN: {PRETRAIN_MAX_LEN}\")\n",
        "\n",
        "def encode_smiles(smiles, max_len=PRETRAIN_MAX_LEN):\n",
        "    tokens = [SOS_TOKEN] + [c for c in smiles[:max_len-2] if c in token_to_idx] + [EOS_TOKEN]\n",
        "    token_ids = [token_to_idx.get(tok, token_to_idx[PAD_TOKEN]) for tok in tokens]\n",
        "    if len(token_ids) > max_len:\n",
        "        token_ids = token_ids[:max_len]\n",
        "    else:\n",
        "        token_ids += [token_to_idx[PAD_TOKEN]] * (max_len - len(token_ids))\n",
        "    return token_ids\n",
        "\n",
        "# Precompute Morgan fingerprints\n",
        "all_smiles = list(set(df_massspecgym['smiles'].tolist() + df_external['smiles'].tolist()))\n",
        "all_fingerprints = {}\n",
        "morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
        "for smiles in all_smiles:\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol:\n",
        "        all_fingerprints[smiles] = morgan_gen.GetFingerprint(mol)\n",
        "\n",
        "# Dataset class\n",
        "class MSMSDataset(Dataset):\n",
        "    def __init__(self, dataframe, max_len=PRETRAIN_MAX_LEN, is_ssl=False):\n",
        "        self.spectra = np.stack(dataframe['binned'].values)\n",
        "        self.graph_data = dataframe['graph_data'].values\n",
        "        self.ion_modes = dataframe['ion_mode'].values\n",
        "        self.precursor_bins = dataframe['precursor_bin'].values\n",
        "        self.adduct_indices = dataframe['adduct_idx'].values\n",
        "        self.raw_smiles = dataframe['smiles'].values\n",
        "        self.is_ssl = is_ssl\n",
        "        if is_ssl:\n",
        "            self.smiles = []\n",
        "            self.masked_smiles = []\n",
        "            for s in self.raw_smiles:\n",
        "                masked_s, orig_s = self.mask_smiles(s)\n",
        "                self.smiles.append(encode_smiles(orig_s, max_len))\n",
        "                self.masked_smiles.append(encode_smiles(masked_s, max_len))\n",
        "        else:\n",
        "            self.smiles = [encode_smiles(s, max_len=SUPERVISED_MAX_LEN) for s in self.raw_smiles]\n",
        "\n",
        "    def mask_smiles(self, smiles, mask_ratio=0.10):\n",
        "        chars = list(smiles)[:PRETRAIN_MAX_LEN-2]\n",
        "        masked_chars = chars.copy()\n",
        "        n_mask = int(mask_ratio * len(chars))\n",
        "        mask_indices = np.random.choice(len(chars), n_mask, replace=False)\n",
        "        for idx in mask_indices:\n",
        "            masked_chars[idx] = MASK_TOKEN\n",
        "        return ''.join(masked_chars), ''.join(chars)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spectra)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_ssl:\n",
        "            return (\n",
        "                torch.tensor(self.spectra[idx], dtype=torch.float),\n",
        "                self.graph_data[idx],\n",
        "                torch.tensor(self.smiles[idx], dtype=torch.long),\n",
        "                torch.tensor(self.masked_smiles[idx], dtype=torch.long),\n",
        "                torch.tensor(self.ion_modes[idx], dtype=torch.long),\n",
        "                torch.tensor(self.precursor_bins[idx], dtype=torch.long),\n",
        "                torch.tensor(self.adduct_indices[idx], dtype=torch.long),\n",
        "                self.raw_smiles[idx]\n",
        "            )\n",
        "        return (\n",
        "            torch.tensor(self.spectra[idx], dtype=torch.float),\n",
        "            self.graph_data[idx],\n",
        "            torch.tensor(self.smiles[idx], dtype=torch.long),\n",
        "            torch.tensor(self.ion_modes[idx], dtype=torch.long),\n",
        "            torch.tensor(self.precursor_bins[idx], dtype=torch.long),\n",
        "            torch.tensor(self.adduct_indices[idx], dtype=torch.long),\n",
        "            self.raw_smiles[idx]\n",
        "        )\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=1000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "# Transformer Encoder\n",
        "class SpectrumTransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=1000, d_model=768, nhead=12, num_layers=8, dim_feedforward=2048, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.metadata_emb = nn.Linear(2 + 32, 64)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.fc = nn.Linear(d_model + 64, d_model // 2)\n",
        "        self.adduct_emb = nn.Embedding(len(adduct_types), 32)\n",
        "\n",
        "    def forward(self, src, ion_mode_idx, precursor_idx, adduct_idx):\n",
        "        src = self.input_proj(src).unsqueeze(1)\n",
        "        adduct_embed = self.adduct_emb(adduct_idx)\n",
        "        metadata = self.metadata_emb(torch.cat([ion_mode_idx.unsqueeze(-1).float(), precursor_idx.unsqueeze(-1).float(), adduct_embed], dim=-1))\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src).squeeze(1)\n",
        "        output = self.norm(output)\n",
        "        output = torch.cat([output, metadata], dim=-1)\n",
        "        output = self.fc(output)\n",
        "        return output, self.transformer_encoder.layers[-1].self_attn(src, src, src)[1]\n",
        "\n",
        "# GNN Encoder with Expanded Substructures\n",
        "class SpectrumGNNEncoder(MessagePassing):\n",
        "    def __init__(self, d_model=768, hidden_dim=256, num_layers=3, dropout=0.2):\n",
        "        super().__init__(aggr='mean')\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.input_proj = nn.Linear(1, hidden_dim)\n",
        "        self.message_nets = nn.ModuleList([nn.Linear(2 * hidden_dim, hidden_dim) for _ in range(num_layers)])\n",
        "        self.update_nets = nn.ModuleList([nn.Linear(2 * hidden_dim, hidden_dim) for _ in range(num_layers)])\n",
        "        self.metadata_emb = nn.Linear(2 + 32, hidden_dim)\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "        self.output_layer = nn.Linear(hidden_dim, d_model // 2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.substructure_head = nn.Linear(hidden_dim, 30)  # 30 substructures\n",
        "        self.adduct_emb = nn.Embedding(len(adduct_types), 32)\n",
        "        self.substructures = ['C=O', 'C=C', 'c1ccccc1', 'C#N', 'C(=O)O', 'N=O', 'S=O', 'P=O', 'C#C', 'C-N-C',\n",
        "                              'C-O-C', 'C-S-C', 'C(=O)N', 'C(=O)S', 'C=C-C', 'c1ccncc1', 'c1cncnc1', 'c1ccoc1',\n",
        "                              'c1ccsc1', 'C(=O)C', 'N-C-N', 'S-C-S', 'P-C-P', 'C-F', 'C-Cl', 'C-Br', 'C-I', 'N-N',\n",
        "                              'O-O', 'S-S']\n",
        "\n",
        "    def forward(self, graph_data, ion_mode_idx, precursor_idx, adduct_idx):\n",
        "        batch = Batch.from_data_list(graph_data).to(device)\n",
        "        x, edge_index = batch.x, batch.edge_index\n",
        "        ion_mode = batch.ion_mode\n",
        "        precursor_mz = batch.precursor_mz\n",
        "        adduct_embed = self.adduct_emb(adduct_idx)\n",
        "\n",
        "        x = self.input_proj(x)\n",
        "        metadata = self.metadata_emb(torch.cat([ion_mode.unsqueeze(-1), precursor_mz.unsqueeze(-1), adduct_embed], dim=-1))\n",
        "\n",
        "        edge_weights = []\n",
        "        for i in range(self.num_layers):\n",
        "            self._propagate_layer = i\n",
        "            x_before = x.clone()\n",
        "            x = self.propagate(edge_index, x=x)\n",
        "            x = self.update_nets[i](torch.cat([x, metadata], dim=-1))\n",
        "            x = self.norm(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "            edge_weights.append((x - x_before).norm(dim=-1))\n",
        "\n",
        "        x = global_mean_pool(x, batch.batch)\n",
        "        substructure_pred = self.substructure_head(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x, substructure_pred, edge_weights\n",
        "\n",
        "    def message(self, x_i, x_j):\n",
        "        return F.relu(self.message_nets[self._propagate_layer](torch.cat([x_i, x_j], dim=-1)))\n",
        "\n",
        "# Novel Decoder with Stereochemistry and Substructure Guidance\n",
        "class SmilesTransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=768, nhead=12, num_layers=8, dim_feedforward=2048, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "        self.d_model = d_model\n",
        "        self.valence_rules = {\n",
        "            'C': 4, 'N': 3, 'O': 2, 'S': 2, 'P': 3, 'F': 1, 'Cl': 1, 'Br': 1, 'I': 1, 'H': 1\n",
        "        }\n",
        "        self.stereo_tokens = {'@', '/'}\n",
        "        self.substructure_condition = nn.Linear(30, d_model)\n",
        "\n",
        "    def compute_valence(self, smiles_tokens, batch_size):\n",
        "        valence_counts = torch.zeros(batch_size, len(self.valence_rules)).to(smiles_tokens.device)\n",
        "        atom_indices = {tok: i for i, tok in enumerate(self.valence_rules.keys())}\n",
        "        bond_counts = torch.zeros(batch_size, device=smiles_tokens.device)\n",
        "        for t in range(smiles_tokens.size(1)):\n",
        "            for tok, idx in atom_indices.items():\n",
        "                mask = smiles_tokens[:, t] == token_to_idx[tok]\n",
        "                valence_counts[mask, idx] += self.valence_rules[tok]\n",
        "            for tok in ['=', '#']:\n",
        "                mask = smiles_tokens[:, t] == token_to_idx[tok]\n",
        "                bond_counts[mask] += 2 if tok == '#' else 1\n",
        "        valence_counts = valence_counts - bond_counts.unsqueeze(-1)\n",
        "        return torch.relu(valence_counts - torch.tensor(list(self.valence_rules.values()), device=smiles_tokens.device)).sum(dim=-1)\n",
        "\n",
        "    def forward(self, tgt, memory, substructure_pred, tgt_mask=None, memory_key_padding_mask=None):\n",
        "        embedded = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "        embedded = self.pos_encoder(embedded)\n",
        "        substructure_emb = self.substructure_condition(substructure_pred).unsqueeze(1)\n",
        "        embedded = embedded + substructure_emb\n",
        "        output = self.transformer_decoder(embedded, memory, tgt_mask, memory_key_padding_mask)\n",
        "        output = self.norm(output)\n",
        "        logits = self.output_layer(output)\n",
        "        valence_penalty = self.compute_valence(tgt, tgt.size(0))\n",
        "        return logits, valence_penalty\n",
        "\n",
        "# Full Model with RL Component\n",
        "class MSMS2SmilesHybrid(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=768, nhead=12, num_layers=8, dim_feedforward=2048, dropout=0.2, fp_size=2048):\n",
        "        super().__init__()\n",
        "        self.transformer_encoder = SpectrumTransformerEncoder(input_dim=1000, d_model=d_model, nhead=nhead, num_layers=num_layers, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.gnn_encoder = SpectrumGNNEncoder(d_model=d_model, hidden_dim=256, num_layers=3, dropout=dropout)\n",
        "        self.decoder = SmilesTransformerDecoder(vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout)\n",
        "        self.combine_layer = nn.Linear(d_model, d_model)\n",
        "        self.fp_head = nn.Linear(d_model, fp_size)\n",
        "        self.fp_size = fp_size\n",
        "        self.log_sigma_smiles = nn.Parameter(torch.zeros(1))\n",
        "        self.log_sigma_fp = nn.Parameter(torch.zeros(1))\n",
        "        self.log_sigma_sub = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def generate_square_subsequent_mask(self, tgt_len):\n",
        "        mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1)\n",
        "        mask = mask.float().masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, spectrum, graph_data, tgt, ion_mode_idx, precursor_idx, adduct_idx, tgt_mask=None, memory_key_padding_mask=None):\n",
        "        trans_output, attn_weights = self.transformer_encoder(spectrum, ion_mode_idx, precursor_idx, adduct_idx)\n",
        "        gnn_output, substructure_pred, edge_weights = self.gnn_encoder(graph_data, ion_mode_idx, precursor_idx, adduct_idx)\n",
        "        memory = self.combine_layer(torch.cat([trans_output, gnn_output], dim=-1)).unsqueeze(1)\n",
        "        smiles_output, valence_penalty = self.decoder(tgt, memory, substructure_pred, tgt_mask, memory_key_padding_mask)\n",
        "        fp_output = self.fp_head(memory.squeeze(1))\n",
        "        return smiles_output, fp_output, valence_penalty, attn_weights, edge_weights, substructure_pred\n",
        "\n",
        "# SSL Pretraining\n",
        "def ssl_pretrain(model, dataloader, epochs=3, lr=1e-4):\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=token_to_idx[PAD_TOKEN])\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for spectra, graph_data, smiles_tokens, masked_tokens, ion_modes, precursor_bins, adduct_indices, _ in tqdm(dataloader, desc=f\"SSL Epoch {epoch+1}/{epochs}\"):\n",
        "            spectra = spectra.to(device)\n",
        "            ion_modes = ion_modes.to(device)\n",
        "            precursor_bins = precursor_bins.to(device)\n",
        "            adduct_indices = adduct_indices.to(device)\n",
        "            smiles_tokens = smiles_tokens.to(device)\n",
        "            masked_tokens = masked_tokens.to(device)\n",
        "            tgt_input = masked_tokens[:, :-1]\n",
        "            tgt_output = smiles_tokens[:, 1:]\n",
        "            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                smiles_output, _, valence_penalty, _, _, _ = model(spectra, graph_data, tgt_input, ion_modes, precursor_bins, adduct_indices, tgt_mask)\n",
        "                loss = criterion(smiles_output.reshape(-1, vocab_size), tgt_output.reshape(-1)) + 0.1 * valence_penalty.mean()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"SSL Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss\n",
        "        }, f'ssl_checkpoint_epoch_{epoch+1}.pt')\n",
        "        print(f\"Saved SSL checkpoint: ssl_checkpoint_epoch_{epoch+1}.pt\")\n",
        "\n",
        "# Supervised Training with RL\n",
        "def supervised_train(model, train_loader, val_loader, epochs=30, lr=1e-4, patience=5):\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "    smiles_criterion = nn.CrossEntropyLoss(ignore_index=token_to_idx[PAD_TOKEN])\n",
        "    fp_criterion = nn.BCEWithLogitsLoss()\n",
        "    mw_criterion = nn.MSELoss()\n",
        "    sub_criterion = nn.BCEWithLogitsLoss()\n",
        "    best_val_loss = float('inf')\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for spectra, graph_data, smiles_tokens, ion_modes, precursor_bins, adduct_indices, raw_smiles in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            spectra = spectra.to(device)\n",
        "            ion_modes = ion_modes.to(device)\n",
        "            precursor_bins = precursor_bins.to(device)\n",
        "            adduct_indices = adduct_indices.to(device)\n",
        "            smiles_tokens = smiles_tokens.to(device)\n",
        "            tgt_input = smiles_tokens[:, :-1]\n",
        "            tgt_output = smiles_tokens[:, 1:]\n",
        "            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                smiles_output, fp_output, valence_penalty, _, _, substructure_pred = model(spectra, graph_data, tgt_input, ion_modes, precursor_bins, adduct_indices, tgt_mask)\n",
        "                smiles_loss = smiles_criterion(smiles_output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
        "                fp_loss = 0\n",
        "                mw_loss = 0\n",
        "                sub_loss = 0\n",
        "                valid_count = 0\n",
        "                substructure_targets = torch.zeros(len(raw_smiles), 30, dtype=torch.float, device=device)\n",
        "                for i, (smiles, fp) in enumerate(zip(raw_smiles, fp_output)):\n",
        "                    mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
        "                    if mol:\n",
        "                        true_fp = morgan_gen.GetFingerprint(mol)\n",
        "                        fp_loss += fp_criterion(fp, torch.tensor([int(b) for b in true_fp.ToBitString()], dtype=torch.float, device=device))\n",
        "                        mw_loss += mw_criterion(torch.tensor(Descriptors.MolWt(mol), dtype=torch.float, device=device), torch.tensor(500.0, dtype=torch.float, device=device))\n",
        "                        for j, smarts in enumerate(model.gnn_encoder.substructures):\n",
        "                            if mol.HasSubstructMatch(Chem.MolFromSmarts(smarts)):\n",
        "                                substructure_targets[i, j] = 1\n",
        "                        valid_count += 1\n",
        "                fp_loss = fp_loss / valid_count if valid_count > 0 else torch.tensor(0.0, device=device)\n",
        "                mw_loss = mw_loss / valid_count if valid_count > 0 else torch.tensor(0.0, device=device)\n",
        "                sub_loss = sub_criterion(substructure_pred, substructure_targets)\n",
        "                sigma_smiles = torch.clamp(torch.exp(model.log_sigma_smiles), 0.1, 10.0)\n",
        "                sigma_fp = torch.clamp(torch.exp(model.log_sigma_fp), 0.1, 10.0)\n",
        "                sigma_sub = torch.clamp(torch.exp(model.log_sigma_sub), 0.1, 10.0)\n",
        "                supervised_loss = (smiles_loss / (2 * sigma_smiles**2) + model.log_sigma_smiles) + \\\n",
        "                                 (0.1 * fp_loss / (2 * sigma_fp**2) + model.log_sigma_fp) + \\\n",
        "                                 (0.1 * sub_loss / (2 * sigma_sub**2) + model.log_sigma_sub) + \\\n",
        "                                 0.1 * valence_penalty.mean() + 0.1 * mw_loss\n",
        "                # RL component: Tanimoto reward\n",
        "                rl_loss = 0\n",
        "                if epoch >= 5:  # Start RL after initial training\n",
        "                    pred_smiles = beam_search(model, spectra[0], graph_data[0], ion_modes[0], precursor_bins[0], adduct_indices[0], raw_smiles[0], beam_width=5, max_len=SUPERVISED_MAX_LEN, device=device)\n",
        "                    if pred_smiles[0][0] != \"Invalid SMILES\":\n",
        "                        tanimoto = tanimoto_similarity(pred_smiles[0][0], raw_smiles[0], all_fingerprints)\n",
        "                        rl_loss = -torch.log(torch.tensor(tanimoto + 1e-6, device=device))\n",
        "                loss = supervised_loss + 0.1 * rl_loss\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            total_train_loss += loss.item()\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for spectra, graph_data, smiles_tokens, ion_modes, precursor_bins, adduct_indices, raw_smiles in val_loader:\n",
        "                spectra = spectra.to(device)\n",
        "                ion_modes = ion_modes.to(device)\n",
        "                precursor_bins = precursor_bins.to(device)\n",
        "                adduct_indices = adduct_indices.to(device)\n",
        "                smiles_tokens = smiles_tokens.to(device)\n",
        "                tgt_input = smiles_tokens[:, :-1]\n",
        "                tgt_output = smiles_tokens[:, 1:]\n",
        "                tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
        "                with autocast():\n",
        "                    smiles_output, fp_output, valence_penalty, _, _, substructure_pred = model(spectra, graph_data, tgt_input, ion_modes, precursor_bins, adduct_indices, tgt_mask)\n",
        "                    smiles_loss = smiles_criterion(smiles_output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
        "                    fp_loss = 0\n",
        "                    mw_loss = 0\n",
        "                    sub_loss = 0\n",
        "                    valid_count = 0\n",
        "                    substructure_targets = torch.zeros(len(raw_smiles), 30, dtype=torch.float, device=device)\n",
        "                    for i, (smiles, fp) in enumerate(zip(raw_smiles, fp_output)):\n",
        "                        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
        "                        if mol:\n",
        "                            true_fp = morgan_gen.GetFingerprint(mol)\n",
        "                            fp_loss += fp_criterion(fp, torch.tensor([int(b) for b in true_fp.ToBitString()], dtype=torch.float, device=device))\n",
        "                            mw_loss += mw_criterion(torch.tensor(Descriptors.MolWt(mol), dtype=torch.float, device=device), torch.tensor(500.0, dtype=torch.float, device=device))\n",
        "                            for j, smarts in enumerate(model.gnn_encoder.substructures):\n",
        "                                if mol.HasSubstructMatch(Chem.MolFromSmarts(smarts)):\n",
        "                                    substructure_targets[i, j] = 1\n",
        "                            valid_count += 1\n",
        "                    fp_loss = fp_loss / valid_count if valid_count > 0 else torch.tensor(0.0, device=device)\n",
        "                    mw_loss = mw_loss / valid_count if valid_count > 0 else torch.tensor(0.0, device=device)\n",
        "                    sub_loss = sub_criterion(substructure_pred, substructure_targets)\n",
        "                    sigma_smiles = torch.clamp(torch.exp(model.log_sigma_smiles), 0.1, 10.0)\n",
        "                    sigma_fp = torch.clamp(torch.exp(model.log_sigma_fp), 0.1, 10.0)\n",
        "                    sigma_sub = torch.clamp(torch.exp(model.log_sigma_sub), 0.1, 10.0)\n",
        "                    loss = (smiles_loss / (2 * sigma_smiles**2) + model.log_sigma_smiles) + \\\n",
        "                           (0.1 * fp_loss / (2 * sigma_fp**2) + model.log_sigma_fp) + \\\n",
        "                           (0.1 * sub_loss / (2 * sigma_sub**2) + model.log_sigma_sub) + \\\n",
        "                           0.1 * valence_penalty.mean() + 0.1 * mw_loss\n",
        "                total_val_loss += loss.item()\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            checkpoint_path = f'checkpoint_epoch_{epoch+1}.pt'\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'val_loss': avg_val_loss,\n",
        "                'token_to_idx': token_to_idx,\n",
        "                'idx_to_token': idx_to_token\n",
        "            }, checkpoint_path)\n",
        "            print(f\"Saved checkpoint: {checkpoint_path}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            no_improve = 0\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'token_to_idx': token_to_idx,\n",
        "                'idx_to_token': idx_to_token\n",
        "            }, 'best_msms_hybrid.pt')\n",
        "        else:\n",
        "            no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    return best_val_loss\n",
        "\n",
        "# SMILES Syntax Validator\n",
        "def is_valid_smiles_syntax(smiles):\n",
        "    stack = []\n",
        "    for c in smiles:\n",
        "        if c in '([':\n",
        "            stack.append(c)\n",
        "        elif c == ')':\n",
        "            if not stack or stack[-1] != '(':\n",
        "                return False\n",
        "            stack.pop()\n",
        "        elif c == ']':\n",
        "            if not stack or stack[-1] != '[':\n",
        "                return False\n",
        "            stack.pop()\n",
        "    if stack:\n",
        "        return False\n",
        "    i = 0\n",
        "    while i < len(smiles):\n",
        "        if smiles[i] == '[':\n",
        "            j = smiles.find(']', i)\n",
        "            if j == -1:\n",
        "                return False\n",
        "            atom = smiles[i+1:j]\n",
        "            if not any(a in atom for a in valid_atoms):\n",
        "                return False\n",
        "            i = j + 1\n",
        "        else:\n",
        "            if smiles[i] in valid_atoms or smiles[i] in '()=#/\\\\@.:':\n",
        "                i += 1\n",
        "            else:\n",
        "                return False\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
        "        return mol is not None\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# RDKit-based Molecular Property Filter\n",
        "def is_plausible_molecule(smiles, true_mol, max_mw=1500, min_logp=-7, max_logp=7):\n",
        "    mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
        "    if not mol or not is_valid_smiles_syntax(smiles):\n",
        "        return False\n",
        "    mw = Descriptors.MolWt(mol)\n",
        "    logp = Descriptors.MolLogP(mol)\n",
        "    true_mw = Descriptors.MolWt(true_mol) if true_mol else 500\n",
        "    return mw <= max_mw and min_logp <= logp <= max_logp and abs(mw - true_mw) < 300\n",
        "\n",
        "# Evaluation Metrics\n",
        "def dice_similarity(smiles1, smiles2):\n",
        "    mol1 = Chem.MolFromSmiles(smiles1)\n",
        "    mol2 = Chem.MolFromSmiles(smiles2)\n",
        "    if mol1 and mol2:\n",
        "        fp1 = morgan_gen.GetFingerprint(mol1)\n",
        "        fp2 = morgan_gen.GetFingerprint(mol2)\n",
        "        return DataStructs.DiceSimilarity(fp1, fp2)\n",
        "    return 0.0\n",
        "\n",
        "def mcs_similarity(true_smiles, pred_smiles):\n",
        "    mol1 = Chem.MolFromSmiles(true_smiles)\n",
        "    mol2 = Chem.MolFromSmiles(pred_smiles)\n",
        "    if mol1 and mol2:\n",
        "        mcs = rdFMCS.FindMCS([mol1, mol2], timeout=30)\n",
        "        return mcs.numAtoms / max(mol1.GetNumAtoms(), mol2.GetNumAtoms())\n",
        "    return 0.0\n",
        "\n",
        "def mw_difference(true_smiles, pred_smiles):\n",
        "    mol1 = Chem.MolFromSmiles(true_smiles)\n",
        "    mol2 = Chem.MolFromSmiles(pred_smiles)\n",
        "    if mol1 and mol2:\n",
        "        return abs(Descriptors.MolWt(mol1) - Descriptors.MolWt(mol2))\n",
        "    return float('inf')\n",
        "\n",
        "def logp_difference(true_smiles, pred_smiles):\n",
        "    mol1 = Chem.MolFromSmiles(true_smiles)\n",
        "    mol2 = Chem.MolFromSmiles(pred_smiles)\n",
        "    if mol1 and mol2:\n",
        "        return abs(Descriptors.MolLogP(mol1) - Descriptors.MolLogP(mol2))\n",
        "    return float('inf')\n",
        "\n",
        "def substructure_match(true_smiles, pred_smiles, substructures):\n",
        "    mol1 = Chem.MolFromSmiles(true_smiles)\n",
        "    mol2 = Chem.MolFromSmiles(pred_smiles)\n",
        "    if not mol1 or not mol2:\n",
        "        return 0\n",
        "    matches = 0\n",
        "    for smarts in substructures:\n",
        "        pattern = Chem.MolFromSmarts(smarts)\n",
        "        if mol1.HasSubstructMatch(pattern) and mol2.HasSubstructMatch(pattern):\n",
        "            matches += 1\n",
        "    return matches / len(substructures)\n",
        "\n",
        "def validity_rate(pred_smiles_list):\n",
        "    valid = sum(1 for smiles in pred_smiles_list if Chem.MolFromSmiles(smiles, sanitize=True) is not None)\n",
        "    return valid / len(pred_smiles_list) * 100\n",
        "\n",
        "def tanimoto_similarity(smiles1, smiles2, precomputed_fps=None):\n",
        "    mol1 = Chem.MolFromSmiles(smiles1, sanitize=True)\n",
        "    if not mol1:\n",
        "        return 0.0\n",
        "    fp1 = morgan_gen.GetFingerprint(mol1)\n",
        "    if precomputed_fps and smiles2 in precomputed_fps:\n",
        "        fp2 = precomputed_fps[smiles2]\n",
        "    else:\n",
        "        mol2 = Chem.MolFromSmiles(smiles2, sanitize=True)\n",
        "        if not mol2:\n",
        "            return 0.0\n",
        "        fp2 = morgan_gen.GetFingerprint(mol2)\n",
        "    return DataStructs.TanimotoSimilarity(fp1, fp2)\n",
        "\n",
        "def prediction_diversity(pred_smiles_list):\n",
        "    if len(pred_smiles_list) < 2:\n",
        "        return 0.0\n",
        "    total_tanimoto = 0\n",
        "    count = 0\n",
        "    for i in range(len(pred_smiles_list)):\n",
        "        for j in range(i+1, len(pred_smiles_list)):\n",
        "            total_tanimoto += tanimoto_similarity(pred_smiles_list[i], pred_smiles_list[j])\n",
        "            count += 1\n",
        "    return 1 - (total_tanimoto / count) if count > 0 else 0.0\n",
        "\n",
        "# Beam Search with Stereochemistry\n",
        "def beam_search(model, spectrum, graph_data, ion_mode_idx, precursor_idx, adduct_idx, true_smiles, beam_width=10, max_len=150, nucleus_p=0.9, device='cpu'):\n",
        "    model.eval()\n",
        "    true_mol = Chem.MolFromSmiles(true_smiles) if true_smiles else None\n",
        "    with torch.no_grad():\n",
        "        spectrum = spectrum.unsqueeze(0).to(device)\n",
        "        graph_data = Batch.from_data_list([graph_data]).to(device)\n",
        "        ion_mode_idx = torch.tensor([ion_mode_idx], dtype=torch.long).to(device)\n",
        "        precursor_idx = torch.tensor([precursor_idx], dtype=torch.long).to(device)\n",
        "        adduct_idx = torch.tensor([adduct_idx], dtype=torch.long).to(device)\n",
        "        memory = model.transformer_encoder(spectrum, ion_mode_idx, precursor_idx, adduct_idx)[0]\n",
        "        gnn_output, substructure_pred, _ = model.gnn_encoder(graph_data, ion_mode_idx, precursor_idx, adduct_idx)\n",
        "        memory = model.combine_layer(torch.cat([memory, gnn_output], dim=-1)).unsqueeze(1)\n",
        "        sequences = [([token_to_idx[SOS_TOKEN]], 0.0)]\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            all_candidates = []\n",
        "            for seq, score in sequences:\n",
        "                if seq[-1] == token_to_idx[EOS_TOKEN]:\n",
        "                    all_candidates.append((seq, score))\n",
        "                    continue\n",
        "                partial_smiles = ''.join([idx_to_token.get(idx, '') for idx in seq[1:]])\n",
        "                if not is_valid_smiles_syntax(partial_smiles):\n",
        "                    continue\n",
        "                tgt_input = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "                tgt_mask = model.generate_square_subsequent_mask(len(seq)).to(device)\n",
        "                outputs, valence_penalty = model.decoder(tgt_input, memory, substructure_pred, tgt_mask)\n",
        "                log_probs = F.log_softmax(outputs[0, -1], dim=-1).cpu().numpy() - 0.1 * valence_penalty.cpu().numpy()\n",
        "                # Boost stereochemistry tokens\n",
        "                for tok in ['@', '/']:\n",
        "                    if tok in token_to_idx:\n",
        "                        log_probs[token_to_idx[tok]] += 0.5\n",
        "                sorted_probs = np.sort(np.exp(log_probs))[::-1]\n",
        "                cumulative_probs = np.cumsum(sorted_probs)\n",
        "                cutoff_idx = np.searchsorted(cumulative_probs, nucleus_p)\n",
        "                top_tokens = np.argsort(log_probs)[-cutoff_idx:] if cutoff_idx > 0 else np.argsort(log_probs)[-1:]\n",
        "                top_probs = np.exp(log_probs[top_tokens]) / np.sum(np.exp(log_probs[top_tokens]))\n",
        "                for tok in np.random.choice(top_tokens, size=min(beam_width, len(top_tokens)), p=top_probs):\n",
        "                    new_smiles = partial_smiles + idx_to_token.get(int(tok), '')\n",
        "                    if is_valid_smiles_syntax(new_smiles):\n",
        "                        diversity_penalty = 0.2 * sum(1 for s, _ in sequences if tok in s[1:-1])\n",
        "                        all_candidates.append((seq + [int(tok)], score + log_probs[tok] - diversity_penalty))\n",
        "            sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "            if all(seq[-1] == token_to_idx[EOS_TOKEN] for seq, _ in sequences):\n",
        "                break\n",
        "\n",
        "        results = []\n",
        "        for seq, score in sequences:\n",
        "            smiles = ''.join([idx_to_token.get(idx, '') for idx in seq[1:-1]])\n",
        "            try:\n",
        "                mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
        "                if mol and is_plausible_molecule(smiles, true_mol):\n",
        "                    smiles = Chem.MolToSmiles(mol, canonical=True, doRandom=True)\n",
        "                    confidence = np.exp(score / len(seq))\n",
        "                    results.append((smiles, confidence))\n",
        "            except:\n",
        "                continue\n",
        "        return results if results else [(\"Invalid SMILES\", 0.0)]\n",
        "\n",
        "# Visualization Functions\n",
        "def plot_attention_weights(attn_weights, title=\"Transformer Attention Weights\"):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(attn_weights.squeeze().cpu().numpy(), cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Key Tokens\")\n",
        "    plt.ylabel(\"Query Tokens\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_gnn_edge_weights(edge_weights, edge_index, title=\"GNN Edge Importance\"):\n",
        "    edge_scores = edge_weights[-1].cpu().numpy()\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.hist(edge_scores, bins=50)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Edge Weight Magnitude\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "# Error Analysis\n",
        "def error_analysis(pred_smiles_list, true_smiles_list, adducts, precomputed_fps):\n",
        "    errors = {'small': 0, 'large': 0, 'aromatic': 0, 'aliphatic': 0}\n",
        "    adduct_errors = {adduct: [] for adduct in adduct_types}\n",
        "    for pred_smiles, true_smiles, adduct in zip(pred_smiles_list, true_smiles_list, adducts):\n",
        "        tanimoto = tanimoto_similarity(pred_smiles, true_smiles, precomputed_fps)\n",
        "        if tanimoto < 0.3:\n",
        "            mol = Chem.MolFromSmiles(true_smiles)\n",
        "            if mol:\n",
        "                mw = Descriptors.MolWt(mol)\n",
        "                is_aromatic = any(atom.GetIsAromatic() for atom in mol.GetAtoms())\n",
        "                errors['small' if mw < 300 else 'large'] += 1\n",
        "                errors['aromatic' if is_aromatic else 'aliphatic'] += 1\n",
        "                adduct_errors[adduct].append(tanimoto)\n",
        "    print(\"Error Analysis:\")\n",
        "    print(f\"Small molecules (<300 Da) errors: {errors['small']}\")\n",
        "    print(f\"Large molecules (≥300 Da) errors: {errors['large']}\")\n",
        "    print(f\"Aromatic molecule errors: {errors['aromatic']}\")\n",
        "    print(f\"Aliphatic molecule errors: {errors['aliphatic']}\")\n",
        "    for adduct, scores in adduct_errors.items():\n",
        "        if scores:\n",
        "            print(f\"Adduct {adduct} - Avg Tanimoto: {np.mean(scores):.4f}, Count: {len(scores)}\")\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "def objective(trial, train_data, val_data):\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
        "    train_dataset = MSMSDataset(train_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
        "    val_dataset = MSMSDataset(val_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, num_workers=2)\n",
        "    model = MSMS2SmilesHybrid(vocab_size=vocab_size, d_model=768, nhead=12, num_layers=8, dim_feedforward=2048, dropout=0.2, fp_size=2048).to(device)\n",
        "    return supervised_train(model, train_loader, val_loader, epochs=10, lr=lr)\n",
        "\n",
        "# Cross-Validation and Training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "external_dataset = MSMSDataset(df_external, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
        "external_loader = DataLoader(external_dataset, batch_size=32, num_workers=2)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(df_massspecgym)):\n",
        "    print(f\"\\nFold {fold+1}/5\")\n",
        "    train_data = df_massspecgym.iloc[train_idx]\n",
        "    val_data = df_massspecgym.iloc[val_idx]\n",
        "    ssl_data = train_data.sample(frac=0.3, random_state=42)\n",
        "\n",
        "    train_dataset = MSMSDataset(train_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
        "    val_dataset = MSMSDataset(val_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
        "    ssl_dataset = MSMSDataset(ssl_data, max_len=PRETRAIN_MAX_LEN, is_ssl=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, num_workers=2)\n",
        "    ssl_loader = DataLoader(ssl_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(lambda trial: objective(trial, train_data, val_data), n_trials=10)\n",
        "    best_lr = study.best_params['lr']\n",
        "    print(f\"Best learning rate for fold {fold+1}: {best_lr:.6f}\")\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = MSMS2SmilesHybrid(vocab_size=vocab_size, d_model=768, nhead=12, num_layers=8, dim_feedforward=2048, dropout=0.2, fp_size=2048).to(device)\n",
        "    print(f\"Starting SSL pretraining for fold {fold+1}...\")\n",
        "    ssl_pretrain(model, ssl_loader, epochs=3, lr=best_lr)\n",
        "    print(f\"Starting supervised training for fold {fold+1}...\")\n",
        "    best_val_loss = supervised_train(model, train_loader, val_loader, epochs=30, lr=best_lr, patience=5)\n",
        "    fold_results.append(best_val_loss)\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'token_to_idx': token_to_idx,\n",
        "        'idx_to_token': idx_to_token\n",
        "    }, f'best_msms_hybrid_fold_{fold+1}.pt')\n",
        "\n",
        "print(f\"Cross-validation results: {fold_results}\")\n",
        "print(f\"Average validation loss: {np.mean(fold_results):.4f}\")\n",
        "\n",
        "# External Dataset Evaluation\n",
        "model.eval()\n",
        "external_metrics = {'tanimoto': [], 'dice': [], 'mcs': [], 'mw_diff': [], 'logp_diff': [], 'substructure': []}\n",
        "pred_smiles_list = []\n",
        "true_smiles_list = []\n",
        "adducts_list = []\n",
        "num_samples = min(5, len(external_dataset))\n",
        "\n",
        "for sample_idx in range(num_samples):\n",
        "    sample_spectrum = external_dataset[sample_idx][0]\n",
        "    sample_graph = external_dataset[sample_idx][1]\n",
        "    sample_ion_mode = external_dataset[sample_idx][3]\n",
        "    sample_precursor_bin = external_dataset[sample_idx][4]\n",
        "    sample_adduct_idx = external_dataset[sample_idx][5]\n",
        "    true_smiles = external_dataset[sample_idx][6]\n",
        "\n",
        "    predicted_results = beam_search(model, sample_spectrum, sample_graph, sample_ion_mode, sample_precursor_bin, sample_adduct_idx, true_smiles, beam_width=10, max_len=SUPERVISED_MAX_LEN, device=device)\n",
        "    pred_smiles_list.extend([smiles for smiles, _ in predicted_results])\n",
        "    true_smiles_list.extend([true_smiles] * len(predicted_results))\n",
        "    adducts_list.extend([df_external.iloc[sample_idx]['adduct']] * len(predicted_results))\n",
        "\n",
        "    print(f\"\\nExternal Sample {sample_idx} - True SMILES: {true_smiles}\")\n",
        "    print(\"Top Predicted SMILES:\")\n",
        "    for smiles, confidence in predicted_results[:3]:\n",
        "        external_metrics['tanimoto'].append(tanimoto_similarity(smiles, true_smiles, all_fingerprints))\n",
        "        external_metrics['dice'].append(dice_similarity(smiles, true_smiles))\n",
        "        external_metrics['mcs'].append(mcs_similarity(smiles, true_smiles))\n",
        "        external_metrics['mw_diff'].append(mw_difference(smiles, true_smiles))\n",
        "        external_metrics['logp_diff'].append(logp_difference(smiles, true_smiles))\n",
        "        external_metrics['substructure'].append(substructure_match(smiles, true_smiles, model.gnn_encoder.substructures))\n",
        "        print(f\"SMILES: {smiles}, Confidence: {confidence:.4f}, Tanimoto: {external_metrics['tanimoto'][-1]:.4f}, Dice: {external_metrics['dice'][-1]:.4f}, MCS: {external_metrics['mcs'][-1]:.4f}\")\n",
        "        if len(smiles) > 100 and smiles.count('C') > len(smiles) * 0.8:\n",
        "            print(\"Warning: Predicted SMILES is a long carbon chain, indicating potential model underfitting.\")\n",
        "        if smiles != \"Invalid SMILES\":\n",
        "            mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
        "            if mol:\n",
        "                print(f\"Molecular Weight: {Descriptors.MolWt(mol):.2f}, LogP: {Descriptors.MolLogP(mol):.2f}\")\n",
        "\n",
        "    # Visualize molecules\n",
        "    if predicted_results[0][0] != \"Invalid SMILES\":\n",
        "        pred_mol = Chem.MolFromSmiles(predicted_results[0][0], sanitize=True)\n",
        "        true_mol = Chem.MolFromSmiles(true_smiles, sanitize=True)\n",
        "        if pred_mol and true_mol:\n",
        "            img = Draw.MolsToGridImage([true_mol, pred_mol], molsPerRow=2, subImgSize=(300, 300), legends=['True', 'Predicted'])\n",
        "            img_array = np.array(img.convert('RGB'))\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.imshow(img_array)\n",
        "            plt.axis('off')\n",
        "            plt.title(f\"External Sample {sample_idx} - Tanimoto: {external_metrics['tanimoto'][0]:.4f}\")\n",
        "            plt.show()\n",
        "\n",
        "    # Visualize attention and GNN weights for first sample\n",
        "    if sample_idx == 0:\n",
        "        with torch.no_grad():\n",
        "            spectrum = sample_spectrum.unsqueeze(0).to(device)\n",
        "            graph_data = Batch.from_data_list([sample_graph]).to(device)\n",
        "            ion_mode_idx = torch.tensor([sample_ion_mode], dtype=torch.long).to(device)\n",
        "            precursor_idx = torch.tensor([sample_precursor_bin], dtype=torch.long).to(device)\n",
        "            adduct_idx = torch.tensor([sample_adduct_idx], dtype=torch.long).to(device)\n",
        "            _, attn_weights = model.transformer_encoder(spectrum, ion_mode_idx, precursor_idx, adduct_idx)\n",
        "            _, _, edge_weights = model.gnn_encoder(graph_data, ion_mode_idx, precursor_idx, adduct_idx)\n",
        "            plot_attention_weights(attn_weights, title=f\"External Fold Transformer Attention Weights\")\n",
        "            plot_gnn_edge_weights(edge_weights, sample_graph.edge_index, title=f\"External Fold GNN Edge Importance\")\n",
        "\n",
        "# Final Evaluation\n",
        "print(f\"External Validity Rate: {validity_rate(pred_smiles_list):.2f}%\")\n",
        "print(f\"External Prediction Diversity: {prediction_diversity(pred_smiles_list):.4f}\")\n",
        "print(\"External Metrics Summary:\")\n",
        "print(f\"Avg Tanimoto: {np.mean(external_metrics['tanimoto']):.4f}\")\n",
        "print(f\"Avg Dice: {np.mean(external_metrics['dice']):.4f}\")\n",
        "print(f\"Avg MCS: {np.mean(external_metrics['mcs']):.4f}\")\n",
        "print(f\"Avg MW Difference: {np.mean([x for x in external_metrics['mw_diff'] if x != float('inf')]):.2f}\")\n",
        "print(f\"Avg LogP Difference: {np.mean([x for x in external_metrics['logp_diff'] if x != float('inf')]):.2f}\")\n",
        "print(f\"Avg Substructure Match: {np.mean(external_metrics['substructure']):.4f}\")\n",
        "error_analysis(pred_smiles_list, true_smiles_list, adducts_list, all_fingerprints)\n"
      ],
      "metadata": {
        "id": "SIAnnRuExQUs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}