{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c425fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for MS-to-Structure pipeline\n",
    "!pip install torch torch_geometric rdkit-pypi selfies datasets optuna nltk python-Levenshtein tqdm scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c2a40",
   "metadata": {},
   "source": [
    "# MS-to-Structure Deep Learning Pipeline (Jupyter Version)\n",
    "\n",
    "This notebook implements a robust mass spectrometry-to-structure (MS-to-structure) deep learning pipeline, adapted for interactive use. It includes data preprocessing, molecular string handling with SELFIES, model definition, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set up logging for Jupyter compatibility\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "from datasets import load_dataset\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors, rdFMCS, EnumerateStereoisomers\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import selfies as sf\n",
    "import optuna\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from Levenshtein import distance\n",
    "import logging\n",
    "import traceback\n",
    "import math\n",
    "\n",
    "# Setup logging for Jupyter (prints to stdout)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s %(message)s'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility and define global variables\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess dataset (replace path as needed)\n",
    "dataset = load_dataset('/kaggle/input/tandem', split='train')\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Simulate external dataset (e.g., NIST-like) by splitting\n",
    "df_massspecgym, df_external = df.iloc[:int(0.9*len(df))], df.iloc[int(0.9*len(df)):]\n",
    "print(\"MassSpecGym size:\", len(df_massspecgym), \"External test size:\", len(df_external))\n",
    "\n",
    "# Inspect dataset\n",
    "print(\"Dataset Columns:\", df_massspecgym.columns.tolist())\n",
    "print(\"\\nFirst few rows of MassSpecGym dataset:\")\n",
    "print(df_massspecgym[['identifier', 'mzs', 'intensities', 'smiles', 'adduct', 'precursor_mz']].head())\n",
    "print(\"\\nUnique adduct values:\", df_massspecgym['adduct'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7780db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonicalize SMILES, augment, and bin spectra\n",
    "def canonicalize_smiles(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol:\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"canonicalize_smiles failed for {smiles}: {e}\\n{traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def augment_smiles(smiles, max_isomers=8):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            opts = EnumerateStereoisomers.EnumerateStereoisomersOptions()\n",
    "            opts.maxIsomers = max_isomers\n",
    "            stereoisomers = EnumerateStereoisomers.EnumerateStereoisomers(mol, options=opts)\n",
    "            return [Chem.MolToSmiles(m, canonical=True, doRandom=True) for m in stereoisomers]\n",
    "        return [smiles]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"augment_smiles failed for {smiles}: {e}\\n{traceback.format_exc()}\")\n",
    "        return [smiles]\n",
    "\n",
    "def bin_spectrum_to_graph(mzs, intensities, ion_mode, precursor_mz, adduct, n_bins=1000, max_mz=1000, noise_level=0.05):\n",
    "    try:\n",
    "        spectrum = np.zeros(n_bins)\n",
    "        for mz, intensity in zip(mzs, intensities):\n",
    "            try:\n",
    "                mz = float(mz)\n",
    "                intensity = float(intensity)\n",
    "                if mz < max_mz:\n",
    "                    bin_idx = int((mz / max_mz) * n_bins)\n",
    "                    spectrum[bin_idx] += intensity\n",
    "            except (ValueError, TypeError) as e:\n",
    "                logging.warning(f\"bin_spectrum_to_graph: Skipping value error: {e}\")\n",
    "                continue\n",
    "        if spectrum.max() > 0:\n",
    "            spectrum = spectrum / spectrum.max()\n",
    "        spectrum += np.random.normal(0, noise_level, spectrum.shape).clip(0, 1)\n",
    "        x = torch.tensor(spectrum, dtype=torch.float).unsqueeze(-1)\n",
    "        edge_index = []\n",
    "        for i in range(n_bins-1):\n",
    "            edge_index.append([i, i+1])\n",
    "            edge_index.append([i+1, i])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "        ion_mode = torch.tensor([ion_mode], dtype=torch.float)\n",
    "        precursor_mz = torch.tensor([precursor_mz], dtype=torch.float)\n",
    "        adduct_idx = adduct_to_idx.get(adduct, 0)\n",
    "        return spectrum, Data(x=x, edge_index=edge_index, ion_mode=ion_mode, precursor_mz=precursor_mz, adduct_idx=adduct_idx)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"bin_spectrum_to_graph failed: {e}\\n{traceback.format_exc()}\")\n",
    "        return np.zeros(n_bins), Data(x=torch.zeros(n_bins, 1), edge_index=torch.zeros(2, 0, dtype=torch.long), ion_mode=torch.zeros(1), precursor_mz=torch.zeros(1), adduct_idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply canonicalization, augmentation, and binning to the dataframe\n",
    "# Preprocess ion mode, precursor m/z, and adducts\n",
    "df_massspecgym['smiles'] = df_massspecgym['smiles'].apply(canonicalize_smiles)\n",
    "df_external['smiles'] = df_external['smiles'].apply(canonicalize_smiles)\n",
    "df_massspecgym = df_massspecgym.dropna(subset=['smiles'])\n",
    "df_external = df_external.dropna(subset=['smiles'])\n",
    "df_massspecgym['smiles_list'] = df_massspecgym['smiles'].apply(augment_smiles)\n",
    "df_massspecgym = df_massspecgym.explode('smiles_list').dropna(subset=['smiles_list']).rename(columns={'smiles_list': 'smiles'})\n",
    "\n",
    "df_massspecgym['ion_mode'] = df_massspecgym['adduct'].apply(lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0).fillna(0)\n",
    "df_massspecgym['precursor_bin'] = pd.qcut(df_massspecgym['precursor_mz'], q=100, labels=False, duplicates='drop')\n",
    "df_external['ion_mode'] = df_external['adduct'].apply(lambda x: 0 if '+' in str(x) else 1 if '-' in str(x) else 0).fillna(0)\n",
    "df_external['precursor_bin'] = pd.qcut(df_external['precursor_mz'], q=100, labels=False, duplicates='drop')\n",
    "adduct_types = df_massspecgym['adduct'].unique()\n",
    "adduct_to_idx = {adduct: i for i, adduct in enumerate(adduct_types)}\n",
    "df_massspecgym['adduct_idx'] = df_massspecgym['adduct'].map(adduct_to_idx)\n",
    "df_external['adduct_idx'] = df_external['adduct'].map(adduct_to_idx)\n",
    "\n",
    "df_massspecgym[['binned', 'graph_data']] = df_massspecgym.apply(\n",
    "    lambda row: pd.Series(bin_spectrum_to_graph(row['mzs'], row['intensities'], row['ion_mode'], row['precursor_mz'], row['adduct'])),\n",
    "    axis=1\n",
    ")\n",
    "df_external[['binned', 'graph_data']] = df_external.apply(\n",
    "    lambda row: pd.Series(bin_spectrum_to_graph(row['mzs'], row['intensities'], row['ion_mode'], row['precursor_mz'], row['adduct'])),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELFIES tokenization and vocabulary setup\n",
    "all_smiles = df_massspecgym['smiles'].tolist()\n",
    "all_selfies = [sf.encoder(s) for s in all_smiles]\n",
    "selfies_alphabet = set()\n",
    "for s in all_selfies:\n",
    "    selfies_alphabet.update(sf.split_selfies(s))\n",
    "selfies_tokens = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, MASK_TOKEN] + sorted(selfies_alphabet)\n",
    "token_to_idx = {tok: i for i, tok in enumerate(selfies_tokens)}\n",
    "idx_to_token = {i: tok for tok, i in token_to_idx.items()}\n",
    "vocab_size = len(token_to_idx)\n",
    "PRETRAIN_MAX_LEN = 100\n",
    "SUPERVISED_MAX_LEN = max(len(sf.split_selfies(s)) + 2 for s in all_selfies)\n",
    "print(f\"SELFIES vocabulary size: {vocab_size}, Supervised MAX_LEN: {SUPERVISED_MAX_LEN}, Pretrain MAX_LEN: {PRETRAIN_MAX_LEN}\")\n",
    "\n",
    "def encode_selfies(selfies, max_len=PRETRAIN_MAX_LEN):\n",
    "    tokens = [SOS_TOKEN] + sf.split_selfies(selfies)[:max_len-2] + [EOS_TOKEN]\n",
    "    token_ids = [token_to_idx.get(tok, token_to_idx[PAD_TOKEN]) for tok in tokens]\n",
    "    if len(token_ids) > max_len:\n",
    "        token_ids = token_ids[:max_len]\n",
    "    else:\n",
    "        token_ids += [token_to_idx[PAD_TOKEN]] * (max_len - len(token_ids))\n",
    "    return token_ids\n",
    "\n",
    "def decode_selfies(token_ids):\n",
    "    tokens = [idx_to_token.get(idx, PAD_TOKEN) for idx in token_ids]\n",
    "    tokens = [t for t in tokens if t not in {PAD_TOKEN, SOS_TOKEN, EOS_TOKEN}]\n",
    "    selfies_str = ''.join(tokens)\n",
    "    try:\n",
    "        smiles = sf.decoder(selfies_str)\n",
    "        return smiles\n",
    "    except Exception:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute Morgan fingerprints for all unique SMILES\n",
    "all_smiles = list(set(df_massspecgym['smiles'].tolist() + df_external['smiles'].tolist()))\n",
    "all_fingerprints = {}\n",
    "morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "for smiles in all_smiles:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        all_fingerprints[smiles] = morgan_gen.GetFingerprint(mol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for MS/MS data\n",
    "class MSMSDataset(Dataset):\n",
    "    def __init__(self, dataframe, max_len=PRETRAIN_MAX_LEN, is_ssl=False):\n",
    "        self.spectra = np.stack(dataframe['binned'].values)\n",
    "        self.graph_data = dataframe['graph_data'].values\n",
    "        self.ion_modes = dataframe['ion_mode'].values\n",
    "        self.precursor_bins = dataframe['precursor_bin'].values\n",
    "        self.adduct_indices = dataframe['adduct_idx'].values\n",
    "        self.raw_smiles = dataframe['smiles'].values\n",
    "        self.is_ssl = is_ssl\n",
    "        if is_ssl:\n",
    "            self.smiles = []\n",
    "            self.masked_smiles = []\n",
    "            for s in self.raw_smiles:\n",
    "                selfies = sf.encoder(s)\n",
    "                masked_s, orig_s = self.mask_selfies(selfies)\n",
    "                self.smiles.append(encode_selfies(orig_s, max_len))\n",
    "                self.masked_smiles.append(encode_selfies(masked_s, max_len))\n",
    "        else:\n",
    "            self.smiles = [encode_selfies(sf.encoder(s), max_len=SUPERVISED_MAX_LEN) for s in self.raw_smiles]\n",
    "\n",
    "    def mask_selfies(self, selfies, mask_ratio=0.10):\n",
    "        try:\n",
    "            tokens = sf.split_selfies(selfies)[:PRETRAIN_MAX_LEN-2]\n",
    "            masked_tokens = tokens.copy()\n",
    "            n_mask = int(mask_ratio * len(tokens))\n",
    "            if n_mask > 0:\n",
    "                mask_indices = np.random.choice(len(tokens), n_mask, replace=False)\n",
    "                for idx in mask_indices:\n",
    "                    masked_tokens[idx] = MASK_TOKEN\n",
    "            return ''.join(masked_tokens), ''.join(tokens)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"mask_selfies failed for {selfies}: {e}\\n{traceback.format_exc()}\")\n",
    "            return selfies, selfies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spectra)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_ssl:\n",
    "            return (\n",
    "                torch.tensor(self.spectra[idx], dtype=torch.float),\n",
    "                self.graph_data[idx],\n",
    "                torch.tensor(self.smiles[idx], dtype=torch.long),\n",
    "                torch.tensor(self.masked_smiles[idx], dtype=torch.long),\n",
    "                torch.tensor(self.ion_modes[idx], dtype=torch.long),\n",
    "                torch.tensor(self.precursor_bins[idx], dtype=torch.long),\n",
    "                torch.tensor(self.adduct_indices[idx], dtype=torch.long),\n",
    "                self.raw_smiles[idx]\n",
    "            )\n",
    "        return (\n",
    "            torch.tensor(self.spectra[idx], dtype=torch.float),\n",
    "            self.graph_data[idx],\n",
    "            torch.tensor(self.smiles[idx], dtype=torch.long),\n",
    "            torch.tensor(self.ion_modes[idx], dtype=torch.long),\n",
    "            torch.tensor(self.precursor_bins[idx], dtype=torch.long),\n",
    "            torch.tensor(self.adduct_indices[idx], dtype=torch.long),\n",
    "            self.raw_smiles[idx]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6336cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding and model encoder/decoder classes\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# (Add SpectrumTransformerEncoder, SpectrumGNNEncoder, SmilesTransformerDecoder, MSMS2SmilesHybrid classes here, as in the script)\n",
    "# For brevity, you can copy the class definitions from the script into this cell or split them into multiple cells if preferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c41cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation utilities (SSL pretrain, supervised train, metrics, beam search, etc)\n",
    "# Copy the relevant functions from the script here, e.g.:\n",
    "# ssl_pretrain, supervised_train, is_valid_smiles_syntax, is_plausible_molecule, dice_similarity, mcs_similarity, mw_difference, logp_difference, substructure_match, validity_rate, tanimoto_similarity, prediction_diversity, beam_search, batch_beam_search, plot_attention_weights, plot_gnn_edge_weights, error_analysis, objective\n",
    "# For brevity, you can split these into multiple cells if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb652e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation, training, and evaluation loop\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "external_dataset = MSMSDataset(df_external, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "external_loader = DataLoader(external_dataset, batch_size=32, num_workers=2)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_massspecgym)):\n",
    "    print(f\"\\nFold {fold+1}/5\")\n",
    "    train_data = df_massspecgym.iloc[train_idx]\n",
    "    val_data = df_massspecgym.iloc[val_idx]\n",
    "    ssl_data = train_data.sample(frac=0.3, random_state=42)\n",
    "\n",
    "    train_dataset = MSMSDataset(train_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "    val_dataset = MSMSDataset(val_data, max_len=SUPERVISED_MAX_LEN, is_ssl=False)\n",
    "    ssl_dataset = MSMSDataset(ssl_data, max_len=PRETRAIN_MAX_LEN, is_ssl=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, num_workers=2)\n",
    "    ssl_loader = DataLoader(ssl_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, train_data, val_data), n_trials=10)\n",
    "    best_lr = study.best_params['lr']\n",
    "    print(f\"Best learning rate for fold {fold+1}: {best_lr:.6f}\")\n",
    "\n",
    "    # Initialize and train model\n",
    "    model = MSMS2SmilesHybrid(vocab_size=vocab_size, d_model=768, nhead=12, num_layers=8, dim_feedforward=2048, dropout=0.2, fp_size=2048).to(device)\n",
    "    print(f\"Starting SSL pretraining for fold {fold+1}...\")\n",
    "    ssl_pretrain(model, ssl_loader, epochs=3, lr=best_lr)\n",
    "    print(f\"Starting supervised training for fold {fold+1}...\")\n",
    "    best_val_loss = supervised_train(model, train_loader, val_loader, epochs=30, lr=best_lr, patience=5)\n",
    "    fold_results.append(best_val_loss)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'token_to_idx': token_to_idx,\n",
    "        'idx_to_token': idx_to_token\n",
    "    }, f'best_msms_hybrid_fold_{fold+1}.pt')\n",
    "\n",
    "print(f\"Cross-validation results: {fold_results}\")\n",
    "print(f\"Average validation loss: {np.mean(fold_results):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dataset evaluation and visualization\n",
    "model.eval()\n",
    "external_metrics = {'tanimoto': [], 'dice': [], 'mcs': [], 'mw_diff': [], 'logp_diff': [], 'substructure': []}\n",
    "pred_smiles_list = []\n",
    "true_smiles_list = []\n",
    "adducts_list = []\n",
    "num_samples = min(5, len(external_dataset))\n",
    "\n",
    "for sample_idx in range(num_samples):\n",
    "    sample_spectrum = external_dataset[sample_idx][0]\n",
    "    sample_graph = external_dataset[sample_idx][1]\n",
    "    sample_ion_mode = external_dataset[sample_idx][3]\n",
    "    sample_precursor_bin = external_dataset[sample_idx][4]\n",
    "    sample_adduct_idx = external_dataset[sample_idx][5]\n",
    "    true_smiles = external_dataset[sample_idx][6]\n",
    "\n",
    "    predicted_results = beam_search(model, sample_spectrum, sample_graph, sample_ion_mode, sample_precursor_bin, sample_adduct_idx, true_smiles, beam_width=10, max_len=SUPERVISED_MAX_LEN, device=device)\n",
    "    pred_smiles_list.extend([smiles for smiles, _ in predicted_results])\n",
    "    true_smiles_list.extend([true_smiles] * len(predicted_results))\n",
    "    adducts_list.extend([df_external.iloc[sample_idx]['adduct']] * len(predicted_results))\n",
    "\n",
    "    print(f\"\\nExternal Sample {sample_idx} - True SMILES: {true_smiles}\")\n",
    "    print(\"Top Predicted SMILES:\")\n",
    "    for smiles, confidence in predicted_results[:3]:\n",
    "        external_metrics['tanimoto'].append(tanimoto_similarity(smiles, true_smiles, all_fingerprints))\n",
    "        external_metrics['dice'].append(dice_similarity(smiles, true_smiles))\n",
    "        external_metrics['mcs'].append(mcs_similarity(smiles, true_smiles))\n",
    "        external_metrics['mw_diff'].append(mw_difference(smiles, true_smiles))\n",
    "        external_metrics['logp_diff'].append(logp_difference(smiles, true_smiles))\n",
    "        external_metrics['substructure'].append(substructure_match(smiles, true_smiles, model.gnn_encoder.substructures))\n",
    "        print(f\"SMILES: {smiles}, Confidence: {confidence:.4f}, Tanimoto: {external_metrics['tanimoto'][-1]:.4f}, Dice: {external_metrics['dice'][-1]:.4f}, MCS: {external_metrics['mcs'][-1]:.4f}\")\n",
    "        if len(smiles) > 100 and smiles.count('C') > len(smiles) * 0.8:\n",
    "            print(\"Warning: Predicted SMILES is a long carbon chain, indicating potential model underfitting.\")\n",
    "        if smiles != \"Invalid SMILES\":\n",
    "            mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "            if mol:\n",
    "                print(f\"Molecular Weight: {Descriptors.MolWt(mol):.2f}, LogP: {Descriptors.MolLogP(mol):.2f}\")\n",
    "\n",
    "    # Visualize molecules\n",
    "    if predicted_results[0][0] != \"Invalid SMILES\":\n",
    "        pred_mol = Chem.MolFromSmiles(predicted_results[0][0], sanitize=True)\n",
    "        true_mol = Chem.MolFromSmiles(true_smiles, sanitize=True)\n",
    "        if pred_mol and true_mol:\n",
    "            img = Draw.MolsToGridImage([true_mol, pred_mol], molsPerRow=2, subImgSize=(300, 300), legends=['True', 'Predicted'])\n",
    "            img_array = np.array(img.convert('RGB'))\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(img_array)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"External Sample {sample_idx} - Tanimoto: {external_metrics['tanimoto'][0]:.4f}\")\n",
    "            plt.show()\n",
    "\n",
    "    # Visualize attention and GNN weights for first sample\n",
    "    if sample_idx == 0:\n",
    "        with torch.no_grad():\n",
    "            spectrum = sample_spectrum.unsqueeze(0).to(device)\n",
    "            graph_data = Batch.from_data_list([sample_graph]).to(device)\n",
    "            ion_mode_idx = torch.tensor([sample_ion_mode], dtype=torch.long).to(device)\n",
    "            precursor_idx = torch.tensor([sample_precursor_bin], dtype=torch.long).to(device)\n",
    "            adduct_idx = torch.tensor([sample_adduct_idx], dtype=torch.long).to(device)\n",
    "            _, attn_weights = model.transformer_encoder(spectrum, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "            _, _, edge_weights = model.gnn_encoder(graph_data, ion_mode_idx, precursor_idx, adduct_idx)\n",
    "            plot_attention_weights(attn_weights, title=f\"External Fold Transformer Attention Weights\")\n",
    "            plot_gnn_edge_weights(edge_weights, sample_graph.edge_index, title=f\"External Fold GNN Edge Importance\")\n",
    "\n",
    "# Final Evaluation\n",
    "print(f\"External Validity Rate: {validity_rate(pred_smiles_list):.2f}%\")\n",
    "print(f\"External Prediction Diversity: {prediction_diversity(pred_smiles_list):.4f}\")\n",
    "print(\"External Metrics Summary:\")\n",
    "print(f\"Avg Tanimoto: {np.mean(external_metrics['tanimoto']):.4f}\")\n",
    "print(f\"Avg Dice: {np.mean(external_metrics['dice']):.4f}\")\n",
    "print(f\"Avg MCS: {np.mean(external_metrics['mcs']):.4f}\")\n",
    "print(f\"Avg MW Difference: {np.mean([x for x in external_metrics['mw_diff'] if x != float('inf')]):.2f}\")\n",
    "print(f\"Avg LogP Difference: {np.mean([x for x in external_metrics['logp_diff'] if x != float('inf')]):.2f}\")\n",
    "print(f\"Avg Substructure Match: {np.mean(external_metrics['substructure']):.4f}\")\n",
    "error_analysis(pred_smiles_list, true_smiles_list, adducts_list, all_fingerprints)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
